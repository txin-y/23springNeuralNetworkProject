{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txin-y/23springNeuralNetworkProject/blob/main/IMDBtextclassification_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRGJ_Y1N7Vld",
        "outputId": "2dd5d390-f97f-4056-a815-e998eff209de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install portalocker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYw-HIym7LSV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "import portalocker\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "train_iter = iter(IMDB(split='train'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR0JtN8M7b8n"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUNkKEzI8M0Y"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9BDex8w8VZZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    # print(label_list.size())\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "num_class = len(set([label for (label, text) in train_iter]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTGa1FUmNkhs"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3oTZPcJapgg"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Text classifier based on a pytorch TransformerEncoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        d_model,\n",
        "        # embeddings,\n",
        "        nhead=4,\n",
        "        dim_feedforward=2048,\n",
        "        num_layers=2,\n",
        "        dropout=0,\n",
        "        activation=\"relu\",\n",
        "        classifier_dropout=0,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # vocab_size, d_model = embeddings.size()\n",
        "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
        "\n",
        "        # self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.emb = nn.EmbeddingBag(vocab_size, d_model)\n",
        "\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=d_model,\n",
        "            dropout=dropout,\n",
        "            # vocab_size=vocab_size,\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.decoder = nn.Linear(d_model, num_class)\n",
        "        self.d_model = d_model\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "\n",
        "    def forward(self, x,offsets):\n",
        "        x = self.emb(x,offsets) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.decoder(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.emb(x) * math.sqrt(self.d_model)\n",
        "    #     x = self.pos_encoder(x)\n",
        "    #     x = self.encoder(x)\n",
        "    #     x = x.mean(dim=1)\n",
        "    #     x = self.decoder(x)\n",
        "    #     x = self.softmax(x)\n",
        "    #     # print('net '+str(x.size()))\n",
        "    #     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM2ju2BXcj1c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "model = Net(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=200,\n",
        "    nhead=2,  # the number of heads in the multiheadattention models\n",
        "    dim_feedforward=2048,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    num_layers=2,\n",
        "    dropout=0.1,\n",
        "    classifier_dropout=0.1,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpPQTqtrVHK6"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, k, heads=4, mask=False):\n",
        "      \n",
        "      super().__init__()\n",
        "      \n",
        "      assert k % heads == 0\n",
        "      \n",
        "      self.k, self.heads = k, heads\n",
        "\n",
        "      # These compute the queries, keys and values for all heads\n",
        "      self.tokeys    = nn.Linear(k, k, bias=False)\n",
        "      self.toqueries = nn.Linear(k, k, bias=False)\n",
        "      self.tovalues  = nn.Linear(k, k, bias=False)\n",
        "\n",
        "      # This will be applied after the multi-head self-attention operation.\n",
        "      self.unifyheads = nn.Linear(k, k)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      b, t, k = x.size()\n",
        "      h = self.heads\n",
        "\n",
        "      queries = self.toqueries(x)\n",
        "      keys    = self.tokeys(x)   \n",
        "      values  = self.tovalues(x)\n",
        "\n",
        "      s = k // h\n",
        "\n",
        "      keys    = keys.view(b, t, h, s)\n",
        "      queries = queries.view(b, t, h, s)\n",
        "      values  = values.view(b, t, h, s)\n",
        "\n",
        "       # - fold heads into the batch dimension\n",
        "      keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "      queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "      values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "      \n",
        "      queries = queries \n",
        "      keys    = keys\n",
        "\n",
        "      # Get dot product of queries and keys, and scale\n",
        "      dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "      # -- dot has size (b*h, t, t) containing raw weights\n",
        "\n",
        "      # scale the dot product\n",
        "      dot = dot / (k ** (1/2))\n",
        "    \n",
        "      # normalize \n",
        "      dot = F.softmax(dot, dim=2)\n",
        "      # - dot now contains row-wise normalized weights\n",
        "\n",
        "      # apply the self attention to the values\n",
        "      out = torch.bmm(dot, values).view(b, h, t, s)\n",
        "      \n",
        "      # swap h, t back, unify heads\n",
        "      out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "    \n",
        "      return self.unifyheads(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTKXSPrQVldw"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = SelfAttention(k, heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "    nn.Linear(k, 4 * k),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4 * k, k))\n",
        "\n",
        "  def forward(self, x):\n",
        "    attended = self.attention(x)\n",
        "    x = self.norm1(attended + x)\n",
        "\n",
        "    fedforward = self.ff(x)\n",
        "    return self.norm2(fedforward + x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2h82qx3O3Ms"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_tokens = num_tokens\n",
        "        self.token_emb = nn.Embedding(num_tokens, k)\n",
        "        self.pos_emb = nn.Embedding(seq_length, k)\n",
        "\n",
        "\t\t# The sequence of transformer blocks that does all the\n",
        "\t\t# heavy lifting\n",
        "        tblocks = []\n",
        "        for i in range(depth):\n",
        "            tblocks.append(TransformerBlock(k=k, heads=heads))\n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "\t\t# Maps the final output sequence to class logits\n",
        "        self.toprobs = nn.Linear(k, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: A (b, t) tensor of integer values representing\n",
        "                  words (in some predetermined vocabulary).\n",
        "        :return: A (b, c) tensor of log-probabilities over the\n",
        "                 classes (where c is the nr. of classes).\n",
        "        \"\"\"\n",
        "\t\t    # generate token embeddings\n",
        "        tokens = self.token_emb(x)\n",
        "        b, t, k = tokens.size()\n",
        "\n",
        "\t\t    # generate position embeddings\n",
        "        positions = torch.arange(t)\n",
        "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
        "\n",
        "        x = tokens + positions\n",
        "        x = self.tblocks(x)\n",
        "\n",
        "        # Average-pool over the t dimension and project to class\n",
        "        # probabilities\n",
        "        x = self.toprobs(x.mean(dim=1))\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7RCib2_P9Mr"
      },
      "outputs": [],
      "source": [
        "# model = Transformer(\n",
        "#     k=200,\n",
        "#     heads=4,\n",
        "#     depth=2,  \n",
        "#     seq_length=len(vocab),\n",
        "#     num_tokens=len(vocab),\n",
        "#     num_classes = 1,\n",
        "# ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAmJJciZaz5k"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # predicted_label = model(text)\n",
        "        predicted_label = model(text,offsets)\n",
        "        predicted_label = torch.tensor(predicted_label.argmax(1),dtype=torch.float32)\n",
        "        # print(predicted_label.shape)\n",
        "        # print(predicted_label.dtype)\n",
        "        # print(label.shape)\n",
        "        # loss = criterion(predicted_label.squeeze(), label.squeeze())\n",
        "        \n",
        "        loss = criterion(predicted_label, label)\n",
        "        # print(loss.item())\n",
        "        loss.requires_grad = True\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label == label).sum().item()\n",
        "        # total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        # if idx % 5 == 0:\n",
        "          # print('pred'+str(predicted_label)+'label:'+str(label))\n",
        "          # print(total_acc/total_count)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            predicted_label = torch.tensor(predicted_label.argmax(1),dtype=torch.float32)\n",
        "            # loss = criterion(predicted_label.squeeze(), label.squeeze())\n",
        "            loss = criterion(predicted_label, label)\n",
        "            # total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_acc += (predicted_label == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKLtrQgz8mdy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 30 # epoch\n",
        "LR = 1  # learning rate\n",
        "BATCH_SIZE = 50 # batch size for training\n",
        "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "criterion = nn.BCELoss()\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "# criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# \n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "# optimizer = torch.optim.Adam(params, LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, *, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = IMDB()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-rqEuuQbpxZ",
        "outputId": "dbfb9452-cbec-49b4-a5a0-2dc9a2986324"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-1d3cfbb61b72>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  predicted_label = torch.tensor(predicted_label.argmax(1),dtype=torch.float32)\n",
            "<ipython-input-28-1d3cfbb61b72>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  predicted_label = torch.tensor(predicted_label.argmax(1),dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 154.02s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 150.50s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 149.17s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 149.61s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 147.11s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 147.93s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 149.04s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 147.62s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 138.46s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 136.69s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  11 | time: 136.22s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  12 | time: 136.30s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  13 | time: 136.84s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  14 | time: 136.47s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  15 | time: 142.19s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  16 | time: 144.36s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  17 | time: 145.82s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  18 | time: 142.69s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  19 | time: 151.70s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  20 | time: 146.70s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  21 | time: 144.12s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  22 | time: 154.06s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  23 | time: 155.46s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  24 | time: 155.44s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  25 | time: 155.46s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  26 | time: 150.26s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  27 | time: 153.90s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  28 | time: 156.07s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  29 | time: 153.53s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  30 | time: 146.30s | valid accuracy    0.500 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxi1g/mecvpt+I28KuVjQq",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}