{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-Gbdv0Wai81uTFFevykGnnQa9lfoeQ3C",
      "authorship_tag": "ABX9TyMO3c2zRFa7Qc+/3291dLJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txin-y/23springNeuralNetworkProject/blob/main/TweetTextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t5lTaqbylEod"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata==0.5.1 transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9oc1tKJqS9D",
        "outputId": "c59caed3-4133-48db-f79e-e8c9a599d70d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.15)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Installing collected packages: tokenizers, portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, torch, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.0\n",
            "    Uninstalling torchdata-0.6.0:\n",
            "      Successfully uninstalled torchdata-0.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torchdata==0.6.0, but you have torchdata 0.5.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.14.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.7.0 tokenizers-0.13.3 torch-1.13.1 torchdata-0.5.1 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "# df = pd.read_csv(\"twitter_sentiment_analysis.csv\")\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "df.to_csv(\"/content/train.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3z_1LA8nuNU",
        "outputId": "050d781b-5d31-4b5e-c99d-9f0cf63185b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Twitter Sentiment Analysis**\n",
        "Detecting hatred tweets, provided by Analytics Vidhya\n",
        "https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
        "\n",
        "## About Dataset\n",
        "\n",
        "### Context\n",
        "\n",
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
        "\n",
        "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
        "\n",
        "### Content\n",
        "\n",
        "Full tweet texts are provided with their labels for training data.\n",
        "Mentioned users' username is replaced with @user.\n",
        "\n",
        "### Acknowledgements\n",
        "\n",
        "Dataset is provided by Analytics Vidhya"
      ],
      "metadata": {
        "id": "ZC4oB_w_S_DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset class\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.df.iloc[index][\"tweet\"]\n",
        "        sentiment = self.df.iloc[index][\"label\"]\n",
        "        sequence = self.tokenizer.encode(text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(sequence, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(sentiment, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Tokenize the text\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pWByl3iIlJ1V"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(len(tokenizer), embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=2, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input_ids):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        pooled = lstm_output[:, -1, :]\n",
        "        dropped = self.dropout(pooled)\n",
        "        logits = self.fc(dropped)\n",
        "        return self.sigmoid(logits)\n",
        "        # return self.fc(lstm_output[:, -1, :])"
      ],
      "metadata": {
        "id": "rSswSwbxlMfZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(embedding_size=64, hidden_size=16).to(device)"
      ],
      "metadata": {
        "id": "QeaHktPOlQb1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    losses = []\n",
        "\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        # print(input_ids.size(0)) 16\n",
        "        # labels = batch[\"labels\"].to(device)\n",
        "        labels = batch[\"labels\"]\n",
        "        labels = labels.type(torch.LongTensor).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        # predicted = torch.round(outputs)\n",
        "        # print(outputs)\n",
        "        # print(labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (outputs.argmax(1) == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "        losses.append(loss.item())\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f} | loss {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count, sum(losses)/ len(losses)))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader): \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            predicted = torch.round(outputs)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.argmax(1) == labels).sum().item()\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "id": "jZOt-7lRsP_E"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 16 # batch size for training\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_accu = None\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "\n",
        "# Define the datasets and dataloaders\n",
        "max_length = 100\n",
        "train_dataset = TwitterDataset(train_df, tokenizer, max_length)\n",
        "test_dataset = TwitterDataset(test_df, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(len(train_dataloader)) #380 bz 64 1519 bz 16\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "        scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "id": "gZS2UXNOtXUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cffd4d03-b950-43ea-ec2f-4554f5a0a737"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1519\n",
            "| epoch   1 |   500/ 1519 batches | accuracy    0.931 | loss    0.383\n",
            "| epoch   1 |  1000/ 1519 batches | accuracy    0.931 | loss    0.382\n",
            "| epoch   1 |  1500/ 1519 batches | accuracy    0.927 | loss    0.384\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 27.87s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1519 batches | accuracy    0.929 | loss    0.384\n",
            "| epoch   2 |  1000/ 1519 batches | accuracy    0.927 | loss    0.385\n",
            "| epoch   2 |  1500/ 1519 batches | accuracy    0.932 | loss    0.384\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 28.47s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "| epoch   3 |  1000/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "| epoch   3 |  1500/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.30s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1519 batches | accuracy    0.930 | loss    0.384\n",
            "| epoch   4 |  1000/ 1519 batches | accuracy    0.933 | loss    0.382\n",
            "| epoch   4 |  1500/ 1519 batches | accuracy    0.928 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 27.97s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1519 batches | accuracy    0.928 | loss    0.386\n",
            "| epoch   5 |  1000/ 1519 batches | accuracy    0.933 | loss    0.383\n",
            "| epoch   5 |  1500/ 1519 batches | accuracy    0.929 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 28.20s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1519 batches | accuracy    0.927 | loss    0.386\n",
            "| epoch   6 |  1000/ 1519 batches | accuracy    0.932 | loss    0.383\n",
            "| epoch   6 |  1500/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 26.18s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1519 batches | accuracy    0.928 | loss    0.386\n",
            "| epoch   7 |  1000/ 1519 batches | accuracy    0.928 | loss    0.385\n",
            "| epoch   7 |  1500/ 1519 batches | accuracy    0.933 | loss    0.384\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 27.47s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1519 batches | accuracy    0.931 | loss    0.382\n",
            "| epoch   8 |  1000/ 1519 batches | accuracy    0.929 | loss    0.383\n",
            "| epoch   8 |  1500/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.76s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1519 batches | accuracy    0.926 | loss    0.387\n",
            "| epoch   9 |  1000/ 1519 batches | accuracy    0.931 | loss    0.384\n",
            "| epoch   9 |  1500/ 1519 batches | accuracy    0.932 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 28.40s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1519 batches | accuracy    0.930 | loss    0.383\n",
            "| epoch  10 |  1000/ 1519 batches | accuracy    0.932 | loss    0.382\n",
            "| epoch  10 |  1500/ 1519 batches | accuracy    0.928 | loss    0.383\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 27.33s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDfnexJLOLXK"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}